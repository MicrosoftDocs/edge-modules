"ID" : "03C07C02-120F-4FD2-965F-BC3B7FB3D780"
"Status": "Active"
"Create Date" : "3/26/2022"
"Review Date" : "3/26/2022"
"Publish Date" : "3/26/2022"
"Reviewer" : "digwig"
"Author" : "mabrigg"
"Service" : "aks-hci"
"Area"  : "known-issues-workload-clusters"
"Tags"  : "['proof', 'concept']"
"Source"  : "bugs"
"Source ID"  : "1"
"Source URL"  : "https://wwww.bugtool.com/1"
"Target URL"  : "https://docs.microsoft.com/page"
"Schema" : "known-issue"
"Issue" : |
  XXX
"Description" : |
  XXX
"Fix" : |
  XXX


## 106 Linux and Windows VMs weren't configured as highly available VMs when scaling a workload cluster
When scaling out a workload cluster, the corresponding Linux and Windows VMs were added as worker nodes, but they weren't configured as highly available VMs. When running the [Get-ClusterGroup](/powershell/module/failoverclusters/get-clustergroup?view=windowsserver2019-ps&preserve-view=true) command, the newly created Linux VM wasn't configured as a Cluster Group.

This is a known issue. After a reboot, the ability to configure VMs as highly available is sometimes lost. The current workaround is to restart `wssdagent` on each of the Azure Stack HCI nodes. 
This works only for new VMs that are generated by creating node pools when performing a scale up operation or when creating new Kubernetes clusters after restarting the `wssdagent` on the nodes. However, you'll have to manually add the existing VMs to the failover cluster. 

When you scale down a cluster, the high availability cluster resources are in a failed state while the VMs are removed. The workaround for this issue is to manually remove the failed resources.
