"ID" : "E2720416-4AAF-4239-8286-3EDE5684450D"
"Status": "Active"
"Create Date" : "3/26/2022"
"Review Date" : "3/26/2022"
"Publish Date" : "3/26/2022"
"Reviewer" : "abha"
"Author" : "mabrigg"
"Service" : "aks-hci"
"Area"  : "known-issues-workload-clusters"
"Tags"  : "['proof', 'concept']"
"Source"  : "bugs"
"Source ID"  : "1"
"Source URL"  : "https://wwww.bugtool.com/1"
"Target URL"  : "https://docs.microsoft.com/page"
"Schema" : "known-issue"
"Issue" : |
  Linux and Windows VMs weren't configured as highly available VMs when scaling a workload cluster
"Description" : |
  When scaling out a workload cluster, the corresponding Linux and Windows VMs were added as worker nodes, but they weren't configured as highly available VMs. When running the [Get-ClusterGroup](/powershell/module/failoverclusters/get-clustergroup?view=windowsserver2019-ps&preserve-view=true) command, the newly created Linux VM wasn't configured as a Cluster Group.
"Fix" : |
  This is a known issue. After a reboot, the ability to configure VMs as highly available is sometimes lost. The current workaround is to restart `wssdagent` on each of the Azure Stack HCI nodes. 
  This works only for new VMs that are generated by creating node pools when performing a scale-out operation or when creating new Kubernetes clusters after restarting the `wssdagent` on the nodes. However, you'll have to manually add the existing VMs to the failover cluster. 

  When you scale down a cluster, the high-availability cluster resources are in a failed state while the VMs are removed. The workaround for this issue is to manually remove the failed resources.