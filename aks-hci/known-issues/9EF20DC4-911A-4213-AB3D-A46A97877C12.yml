"ID" : "9EF20DC4-911A-4213-AB3D-A46A97877C12"
"Status": "Active"
"Create Date" : "04/22/2022"
"Review Date" : "04/22/2022"
"Publish Date" : "04/22/2022"
"Reviewer" : "aweston"
"Author" : "mabrigg"
"Service" : "aks-hci"
"Area"  : "known-issues-upgrade"
"Tags"  : "['proof', 'concept']"
"Source"  : "bugs"
"Source ID"  : "39072488"
"Source URL"  : "https://microsoft.visualstudio.com/OS/_workitems/edit/39072488"
"Target URL"  : "https://docs.microsoft.com/azure-stack/aks-hci/known-issues-upgrade"
"Schema" : "known-issue"
"Issue" : |
  Upgrade of target cluster gets stuck with one or more nodes in an old Kubernetes version
"Description" : |
  After running Update-AksHciCluster, the upgrade process stalls.
"Fix" : |
  Run the following command to check if there is a machine with `PHASE` Deleting that is running the previous Kubernetes version that you are upgrading from.

  ```
  kubectl get machines -o wide --kubeconfig (Get-KvaConfig).kubeconfig
  ```

  If there is a machine with `PHASE` Deleting and `VERSION` matching the previous Kubernetes version, proceed with the following steps.

  To address this problem, you need the following pieces of information:
  1.  Kubernetes version to which you're upgrading your workload cluster.
  2. IP address of the node that is stuck.

  To find this information, run the following cmdlet and command. By default, the cmdlet `Get-AksHciCredential` writes the kubeconfig to `~/.kube/config`. If you specify a different location for your workload cluster kubeconfig when calling `Get-AksHciCredential`, you will need to provide that location to kubectl as another argument. For example, `kubectl get nodes -o wide --kubeconfig <path to workload cluster kubeconfig>`.

  ```powershell
  Get-AksHciCredential -name <workload cluster name>
  kubectl get nodes -o wide
  ```

  The node that needs to be fixed should show `STATUS` `Ready,SchedulingDisabled`. If you see a node with this status, use the `INTERNAL-IP` of this node as the IP address value in the following command below. Use the Kubernetes version you are upgrading to as the version value; this should match the `VERSION` value from the node with `ROLES` `control-plane,master`. Be sure to include the full value for the Kubernetes version, including the `v`, for example `v1.22.6`.

  ```
  ssh -i (Get-MocConfig).sshPrivateKey -o StrictHostKeyChecking=no  clouduser@<IP address> sudo crictl pull ecpacr.azurecr.io/kube-proxy:<Kubernetes version>
  ```

  After running this ssh command, the remaining nodes with the old Kubernetes version should be deleted and the upgrade will proceed.